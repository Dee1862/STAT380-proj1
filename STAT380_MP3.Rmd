---
title: "Mini Project 3"
author: "Deekshith Reddy Bhoomireddy, Milton La"
date: "2024-12-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter - Clean Environment, Load Libraries, User Defined Functions

```{r}
rm(list = ls())
#Add libraries as needed
library(tidyverse)
library(dplyr)
library(glmnet)
library(MASS)
library(readxl)
library(rpart)
library(rattle)

data <- read_xlsx("CODGames2_mp.xlsx")
```

## Task 1

```{r}
fp_full <-
  data |>
  filter(FullPartial == "Full")
```

```{r}
# Side-by-side boxplot for the TotalXP and the XRType
fp_full |>
  ggplot(aes(x = XPType, y = TotalXP)) +
  geom_boxplot() + 
  labs(title = "Boxplot of TotalXP by XPType",
       x = "Type of Experience Points Earned",
       y = "Total Experiece Points (XP)")
```


```{r}
summary_stats <-
  fp_full |>
  group_by(XPType) |>
  summarize(
    Mean = mean(TotalXP),
    Median = median(TotalXP),
    Min = min(TotalXP),
    Max = max(TotalXP),
    SD = sd(TotalXP)
  )
print(summary_stats)
```

### Interpretation about the relationship between XPType and the TotalXP

##### Mean:The mean "TotalXP" for players with Double XP + 10% is almost double that of the players with 10% Boost, which indicates that on average, players using the "Double XP + 10%" bonus earn significantly more experience points than those players using the "10% Boost." The huge difference in mean values suggests that the "Double XP + 10%" bonus has a more substantial effect on player performance in terms of experience points.

##### Median: The median TotalXP values is lightly lower than the mean for both the groups which suggests the presence of some outliers which are on the upper sides. Morever, the gap between the median and mean is more in the "Double XP + 10%" group, which indicates that this group has more extreme upper outliers. These outliers pull the mean higher than the median, suggesting that the "Double XP + 10%" group has a right-skewed distribution with a small number of players earning exceptionally high experience points, thereby raising the overall average of this group.

##### Minimum: The minium TotalXP values in both the XPType groups indicates that even the lowest-scoring player with a "Double XP + 10%" XPType have a higher experience score than the lowest scoring players with "10% Boost"

##### Maximum: The maximum TotalXP values in both the XPType groups indicates that players with a "Double XP + 10%" can have a significantly higher experience score than the players with "10% Boost", mostly due thw doubled experience points combined with the 10% bonus points.

##### Standard Deviation: The standard deviation in both the XPType groups indicates variability in the players experience points. Also, the group with a "Double XP + 10%" show a significantly higher standard deviation than the "10% Boost" group, suggesting greater dispersion in experience points within the "Double XP + 10%" group, whereas consistent performance within the "10% boost" group.

##### To conclude, all this above summary and inferences indicate that the overall performance of the players with "Double XP + 10%" is better than those with the "10% Boost".

## Task 2

#### a. Implement LASSO regression and one other feature selection procedure that we covered in Lectures 14/15. Include relevant plots, a discussion on which value of lambda you selected, the estimated equation from LASSO and the estimated equation from the second method. Discuss/compare the results of LASSO with those of the other method. 

#### Preprocessing data:
```{r}
scores <- strsplit(as.character(fp_full$Result), "-")
Won1 <- numeric(length(scores))

for (i in 1:length(scores)) {
  PlayerScore <- as.numeric(scores[[i]][1])
  OpponentScore <- as.numeric(scores[[i]][2])
  
  if (PlayerScore > OpponentScore) {
    Won1[i] <- 1
  }
  
  else {
    Won1[i] <- 0
  }
  
}

fp_full$Won <- Won1

# Prepare the data for modeling
fp_full <- fp_full %>%
  mutate(XPType = factor(XPType))

X <- model.matrix(Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + Won - 1, data = fp_full)
y <- fp_full$Score
```

```{r}
lasso_model <- cv.glmnet(X, y, alpha = 1)

plot(lasso_model)
abline(v = log(lasso_model$lambda.min), col = "red", lty = 2)
title("LASSO Cross-Validation Curve", line = 2.5)

# Best lambda value
best_lambda <- lasso_model$lambda.min
cat("Best lambda for LASSO: ", best_lambda, "\n")

# Coefficient of best lambda
lasso_coefs <- as.matrix(coef(lasso_model, s = "lambda.min"))
lasso_results <- data.frame(
  Feature = rownames(lasso_coefs)[lasso_coefs != 0],
  Coefficient = lasso_coefs[lasso_coefs != 0]
)

# Remove intercept
lasso_results <- lasso_results[lasso_results$Feature != "(Intercept)", ]

print("LASSO Selected Features and Coefficients:")
print(lasso_results)
```


#### Stepwise Regression (other feature selection procedure)
```{r}
# Full model with predictor
full_model <- lm(Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + Won, data = fp_full)

# Stepwise Regression Model
stepwise_model <- step(full_model, direction = "both")

print(summary(stepwise_model))
```

#### Comparison of LASSO and Stepwise Regression
```{r}
cat("Selected Features Comparison:\n")

cat("LASSO Features:\n")
print(lasso_results$Feature)

cat("Stepwise Regression Selected Features:\n")
print(names(coef(stepwise_model)))
```

##### Discussion:
##### LASSO regression provides the approach with selecting important predictors by giving a penalty to less significant variables, and put their coefficients to close zero. On the other hand, Stepwise Regression use the process of iterative to value the prodictors and build the model. However, sometimes, it contains the variables which is not much important.
##### Both approach identifies similar key predictors. Nevertheless, the LASSO model, it automatically handles the variable selection and regularization, it is useful if the model is running simplicity and avoiding overfitting. Stepwise Regression will be helpful with exploring all the possible related variables including the predictors.

#### b. Build a regression tree for predicting Score using total XP, eliminations, deaths, damage, XPType, and whether the player’s team won. Specify that each node must contain at least 15 observations. Display the tree and report the variables associated with the 3 highest variable importance values. (Include the variable importance values when mentioning the variables.)

```{r}
fp_full$Won <- as.factor(fp_full$Won)
#Perform Training/Validation Split
set.seed(123)
trainInd <- sample(1:nrow(fp_full), floor(0.85*nrow(fp_full)))
set.seed(NULL)

#Create Train and Validations variables
Train <- fp_full[trainInd, ]
Validation <- fp_full[-trainInd, ]
```

```{r}
#Grow the regression tree
set.seed(123)
reg_tree <- rpart(Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + Won, 
                  data = Train, 
                  method = "anova",
                  control = rpart.control(minsplit = 15))
```


```{r}
# Plots the regresion tree
fancyRpartPlot(reg_tree, cex = 0.7)
```


```{r}
# Extract the variable importance from the regression tree
variable_importance <- reg_tree$variable.importance
# Displays the extracted variable importance
print(variable_importance)

# Stores the top most important variables
top_variables <- 
  variable_importance |>
  sort(decreasing = TRUE)
# Stores and displays the top 3 important variables
top_3_variables <- top_variables[1:3]
print(top_3_variables)
```

#### c. When building linear regression models, we often wish to determine which variables are the most important. One way of doing this is to look at the magnitude (absolute value) of the estimated coefficients for the regression model built using standardized inputs (centered to have a mean of 0 and a standard deviation of 1). Based on the variables selected by the other feature selection procedure from part a. (in other words, not the LASSO model), standardize the inputs, build the regression model, report the estimated equation, and report the 3 most important variables based on the magnitude (absolute value) of the estimated coefficients. How does this compare to the most important variables based on the regression tree?

```{r}
# Select Variables
selected_vars <- c("TotalXP", "Eliminations", "Deaths", "Damage", "XPType", "Won")

# Check data
str(fp_full[selected_vars])

# Convert
fp_full$XPType <- as.factor(fp_full$XPType)
removable <- model.matrix(~ XPType - 1, data = fp_full)

# Combine
numeric_vars <- fp_full[, c("TotalXP", "Eliminations", "Deaths", "Damage", "Won")]
numeric_vars <- as.data.frame(lapply(numeric_vars, as.numeric))
input <- cbind(numeric_vars, removable)
str(input)

# Scale
scaled_data <- as.data.frame(scale(input))
scaled_data$Score <- fp_full$Score

# Linear Regression Model
stand_model <- lm(Score ~ ., data = scaled_data)

# Calculation
coefficients <- summary(stand_model)$coefficients
diff <- data.frame(
  Variable = rownames(coefficients)[-1], 
  StandardizedCoefficient = coefficients[-1, "Estimate"], 
  AbsoluteValue = abs(coefficients[-1, "Estimate"])
  )

# Rank
diff <- diff[order(-diff$AbsoluteValue), ]
top_3 <- head(diff, 3)

# Expected Equation
equation <- paste("Score =", paste(paste0(round(coefficients[-1, "Estimate"], 3), " * ", rownames(coefficients)[-1]), collapse = " + "))

print("Expected Equation:")
print(equation)

print("Top 3 Most Important Variables:")
print(top_3)
```

##### The standardized regression model identified Eliminations, Damage, and TotalXP as the most important predictors of Score. They are align with the regression tree’s findings. 
##### Both methods do have similar key variables as an ouput, however, the regression tree may better capture non-linear interactions, even if the regression model provides clear result into the magnitude contribution.
